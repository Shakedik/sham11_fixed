import os
import json
import time
from typing import Optional, List, Dict, Any
from fastapi import APIRouter, HTTPException, Query
from dotenv import load_dotenv
import httpx
from kafka import KafkaProducer
from transformers import pipeline
import torch
import asyncio
import re
from urllib.parse import quote # × ×“×¨×© ×œ×§×™×“×•×“ URL ×©×œ ×”×¤×œ×™×™×¡×”×•×œ×“×¨

# ×™×™×‘×•× ×©×œ ×©×™×¨×•×ª×™ ×”××—×¡×•×Ÿ ×”××§×•××™×™× ×©×œ×š (×™×© ×œ×•×•×“× ×©×”× ×§×™×™××™×)
from .storage_service import save_article, get_articles 

# --- 1. ×˜×¢×™× ×ª ××©×ª× ×™ ×¡×‘×™×‘×” (× ×©××¨ ×–×”×”) ---
load_dotenv(dotenv_path=os.path.join(os.path.dirname(os.path.dirname(__file__)), '.env'))
print("ğŸ”‘ Loaded API key:", os.getenv("NEWSDATA_API_KEY"))

NER_MODEL_NAME = "dslim/bert-base-NER"
router = APIRouter(tags=["news"])

# --- 3. ×”×’×“×¨×ª Kafka Producer (× ×©××¨ ×–×”×”) ---
try:
    producer = KafkaProducer(
        bootstrap_servers="localhost:9092",
        value_serializer=lambda v: json.dumps(v).encode("utf-8"),
        retries=3
    )
    print("âœ… Kafka Producer ×”×ª×—×‘×¨ ×‘×”×¦×œ×—×”")
except Exception as e:
    producer = None

# --- 4. ××ª×—×•×œ ××•×“×œ Hugging Face (× ×©××¨ ×–×”×”) ---
try:
    classifier = pipeline("zero-shot-classification", model="facebook/bart-large-mnli")
except Exception as e:
    classifier = None

try:
    ner_pipeline = pipeline("ner", model=NER_MODEL_NAME, aggregation_strategy="simple")
except Exception as e:
    ner_pipeline = None

# --- 5. ××©×ª× ×™× ×•×§×•× ×¤×™×’×™× (× ×©××¨ ×–×”×”) ---
CATEGORIES = ["Politics", "Business", "Science", "Sports", "Entertainment", "Technology"]
NEWSDATA_API_KEY = os.getenv("NEWSDATA_API_KEY", "")
NEWS_API_URL = "https://newsdata.io/api/1/news"
HTTP_TIMEOUT = 10


# ----------------------------------------------------
# ğŸ“Œ ×¤×•× ×§×¦×™×•×ª ×¢×–×¨ ×œ×—×™×¤×•×© ×ª××•× ×” (Wikimedia & Placeholder)
# ----------------------------------------------------
WIKIMEDIA_API_ENDPOINT = "https://commons.wikimedia.org/w/api.php"

def get_placeholder_url(word: str, entity_type: str) -> str:
    """××—×–×™×¨ URL ×©×œ ×ª××•× ×ª ×¤×œ×™×™×¡×”×•×œ×“×¨ ×“×™× ××™×ª (https://placehold.co)"""
    # ×™×¦×™×¨×ª ×¦×‘×¢ ×¢×œ ×‘×¡×™×¡ ×¡×•×’ ×”×™×©×•×ª
    color_map = {"PER": "007bff", "ORG": "dc3545", "LOC": "28a745", "MISC": "6c757d"}
    color = color_map.get(entity_type, "6c757d")
    
    # ×˜×§×¡×˜ ×©×™×•×¤×™×¢ ×¢×œ ×”×¤×œ×™×™×¡×”×•×œ×“×¨ (×”××™×œ×” ×”×¨××©×•× ×” + ×¡×•×’ ×”×™×©×•×ª)
    text = f"{word.split()[0]} ({entity_type})"
    
    # ×™×¦×™×¨×ª ×”-URL
    return f"https://placehold.co/100x100/{color}/FFFFFF/png?text={quote(text)}"


async def get_entity_image_url_async(word: str, entity_type: str) -> str:
    """××—×¤×© ×ª××•× ×” ×‘×•×•×™×§×™××“×™×”, ×× × ×›×©×œ - ××—×–×™×¨ ×¤×œ×™×™×¡×”×•×œ×“×¨."""
    
    # 1. × ×™×¡×™×•×Ÿ ×—×™×¤×•×© ×‘×•×•×™×§×™××“×™×” (×¢× User-Agent)
    search_term = f"{word} {entity_type.lower()}" 
    params = {
        "action": "query", "format": "json", "prop": "imageinfo", "iiprop": "url",
        "generator": "search", "gsrsearch": search_term, "gsrnamespace": 6, 
        "gsrlimit": 1, "iiurlwidth": 100,
    }
    headers = {'User-Agent': 'Sham11NewsAggregator/1.0 (Contact: your@email.com)'}

    async with httpx.AsyncClient(timeout=5) as client:
        try:
            response = await client.get(WIKIMEDIA_API_ENDPOINT, headers=headers, params=params)
            response.raise_for_status()
            data = response.json()
            
            pages = data.get("query", {}).get("pages", {})
            for page_id in pages:
                image_info = pages[page_id].get("imageinfo", [])
                if image_info:
                    return image_info[0].get("thumburl") # âœ… ×”×¦×œ×™×—: ××—×–×™×¨ URL ××•×•×™×§×™××“×™×”
                    
        except Exception as e:
            print(f"Wikimedia Search failed for {word}: {e}")

    # 2. âœ… ×›×©×œ: ××—×–×™×¨ ×ª××•× ×ª ×¤×œ×™×™×¡×”×•×œ×“×¨ ×›×’×™×‘×•×™ ×—×•×‘×”
    return get_placeholder_url(word, entity_type)


# ----------------------------------------------------
# ğŸ“Œ Endpoint: ×”×‘××ª ×—×“×©×•×ª ××—×¨×•× ×•×ª (×”×¤×•× ×§×¦×™×” ×”××¨×›×–×™×ª)
# ----------------------------------------------------
@router.get("/items/recent")
async def get_recent_items(category: Optional[str] = Query(None)):

    # --- ×‘×“×™×§×•×ª ×ª×§×™× ×•×ª (× ×©××¨ ×–×”×”) ---
    if not NEWSDATA_API_KEY:
       raise HTTPException(status_code=500, detail="Missing NEWSDATA_API_KEY in environment variables.")
    if not classifier: raise HTTPException(status_code=500, detail="Hugging Face Classifier ×œ× ×–××™×Ÿ")
    if not ner_pipeline: raise HTTPException(status_code=500, detail="Hugging Face NER Pipeline ×œ× ×–××™×Ÿ")
    if not producer: raise HTTPException(status_code=500, detail="Kafka Producer ×œ× ×–××™×Ÿ")


    params = {
        "apikey": NEWSDATA_API_KEY, "country": "us", "language": "en",
        "category": category.lower() if category else None, "size": 10,
    }
    params = {k: v for k, v in params.items() if v is not None}

    processed_count = 0
    allArray = []

    # --- ×©×œ×™×¤×ª × ×ª×•× ×™× ××”-API (× ×©××¨ ×–×”×”) ---
    async with httpx.AsyncClient(timeout=HTTP_TIMEOUT) as client:
        try:
            response = await client.get(NEWS_API_URL, params=params)
            response.raise_for_status()
            data = response.json()
        except Exception as e:
            raise HTTPException(status_code=502, detail=f"Failed to connect to NewsData API: {e}")

    # --- ×œ×•×œ××ª ×¢×™×‘×•×“ ×›×ª×‘×•×ª ---
    for it in data.get("results", []):
        text = (it.get("title") or "") + " " + (it.get("description") or "")
        if not text.strip(): continue
            
        # --- ×¡×™×•×•×’ (Classifier) ---
        try:
            classification = classifier(text, CATEGORIES)
            top_category = classification["labels"][0]
        except Exception as e:
            top_category = "General"

        # --- ×–×™×”×•×™ ×™×©×•×™×•×ª (NER) ×•×—×™×¤×•×© ×ª××•× ×” ---
        ner_arr = []
        try:
            ner_results = ner_pipeline(text) 
            
            ner_tasks = []
            for res in ner_results:
                entity_type = res.get("entity_group") or res.get("entity")
                word = res["word"].replace('##', '').strip() 
                
                # ××¨×™×¥ ×—×™×¤×•×© ×¨×§ ×× ×™×© ×™×©×•×ª
                if entity_type in ["PER", "ORG", "LOC", "MISC"]:
                    ner_tasks.append(get_entity_image_url_async(word, entity_type)) 
                else:
                    # ×¢×‘×•×¨ ×™×©×•×™×•×ª ×©××™× ×Ÿ ××¢× ×™×™× ×•×ª, ××©×ª××©×™× ×‘×¤×œ×™×™×¡×”×•×œ×“×¨ ×‘×¡×™×¡×™ ××• ×‘-None
                    ner_tasks.append(asyncio.sleep(0, result=None))
            
            image_urls = await asyncio.gather(*ner_tasks)
            
            # ×‘× ×™×™×ª ×”××¢×¨×š ×”×¡×•×¤×™
            for i, res in enumerate(ner_results):
                clean_word = res["word"].replace('##', '').strip() 
                
                # ×× image_urls[i] ×”×•× None, × ×©×ª××© ×‘-None, ××—×¨×ª ×‘-URL ×©×—×–×¨
                final_url = image_urls[i] if image_urls[i] else get_placeholder_url(clean_word, res.get("entity_group") or "MISC")
                
                ner_arr.append({
                    "entity": res.get("entity_group") or res.get("entity"),
                    "word": clean_word,
                    "score": float(res["score"]),
                    "image_url": final_url # ğŸš¨ ×ª××™×“ ×™×—×–×™×¨ URL (×•×™×§×™××“×™×” ××• ×¤×œ×™×™×¡×”×•×œ×“×¨)
                })

        except Exception as e:
            print(f"×©×’×™××ª ×–×™×”×•×™ ×™×©×•×™×•×ª (NER) ××• ×—×™×¤×•×© ×ª××•× ×”: {e}")

        # --- ×‘× ×™×™×ª ××•×‘×™×™×§×˜ ×”× ×ª×•× ×™× ×”××œ× ---
        item_data = {
            "id": it.get("article_id") or it.get("link"),
            "title": it.get("title") or "",
            "description": it.get("description"),
            "content": it.get("description"),
            "url": it.get("link") or "#",
            "image": it.get("image_url"),
            "source": it.get("source_id"),
            "category": top_category,
            "ner_arr": ner_arr,
        }
        allArray.append(item_data)

        # --- ×©××™×¨×” ×œ-DBaaS ×•×©×œ×™×—×” ×œ-Kafka (× ×©××¨ ×–×”×”) ---
        try:
            save_article(item_data)
        except Exception as db_err:
            pass 

        try:
            producer.send(top_category, value={"id": item_data["id"], "category": top_category})
            processed_count += 1
        except Exception as kafka_err:
            pass

    return {
        "message": f"âœ… ×¢×™×‘×•×“ ×”×¡×ª×™×™× â€” {processed_count} ×›×ª×‘×•×ª × ×©×œ×—×• ×œ-Kafka.",
        "data": allArray,
    }


# ----------------------------------------------------
# ğŸ“Œ Endpoints × ×•×¡×¤×™× (× ×©××¨×™× ×›×¤×™ ×©×”×™×•)
# ----------------------------------------------------

@router.post("/save_article")
def add_article(article: dict):
    save_article(article)
    return {"status": "saved"}

@router.get("/articles/{article_id}")
def get_article_by_id(article_id: str):
    article = get_articles(article_id=article_id)
    if article:
        return article
    raise HTTPException(status_code=404, detail="Article not found in DBaaS")